{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKCrJWpvL7LhJRpKWHvBPJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kapibara3chi/CooccurrenceAndWordCloud/blob/main/CooccurrenceNetworkAndWordCloud.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIt6KDZlOao5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要なライブラリをインポート\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# GInzaのモデルをロード\n",
        "nlp = spacy.load(\"ja_ginza\")\n",
        "\n",
        "# 分析対象テキスト例として、Yahoo!ニュースのタイトルを用意[^3^][3]\n",
        "text = \"\"\"\n",
        "首相「感染拡大防止に全力」　緊急事態宣言延長へ\n",
        "東京五輪、海外観客受け入れ断念　政府、IOCと最終調整\n",
        "「五輪中止も」首相発言に反響　与党内からも「選択肢に」\n",
        "菅首相「大阪モデル」評価　医療提供体制の分析・改善要請\n",
        "「コロナ禍でも安心」　東京・渋谷で自動運転バス、実証実験開始\n",
        "\"\"\"\n",
        "\n",
        "# テキストを文単位に分割し、形態素解析を行う\n",
        "docs = [nlp(sent) for sent in text.split(\"\\n\") if sent]\n",
        "\n",
        "# 名詞と動詞だけを抽出し、分かち書きしたリストを作成\n",
        "words = []\n",
        "for doc in docs:\n",
        "    words.extend([token.lemma_ for token in doc if token.pos_ in [\"NOUN\", \"VERB\"]])\n",
        "\n",
        "# Word Cloudを作成するための辞書を作成（単語と出現回数のペア）\n",
        "word_dict = {}\n",
        "for word in words:\n",
        "    word_dict[word] = word_dict.get(word, 0) + 1\n",
        "\n",
        "# Word Cloudのオブジェクトを作成\n",
        "wc = WordCloud(width=800, height=600, background_color=\"white\", font_path=\"C:\\Windows\\Fonts\\meiryo.ttc\")\n",
        "\n",
        "# Word Cloudに辞書を渡して描画\n",
        "wc.generate_from_frequencies(word_dict)\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(wc)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# 共起ネットワークを作成するための辞書を作成（単語のペアと共起回数のペア）\n",
        "co_dict = {}\n",
        "for doc in docs:\n",
        "    tokens = [token.lemma_ for token in doc if token.pos_ in [\"NOUN\", \"VERB\"]]\n",
        "    for i in range(len(tokens) - 1):\n",
        "        for j in range(i + 1, len(tokens)):\n",
        "            pair = tuple(sorted([tokens[i], tokens[j]]))\n",
        "            co_dict[pair] = co_dict.get(pair, 0) + 1\n",
        "\n",
        "# 共起ネットワークのオブジェクトを作成\n",
        "G = nx.Graph()\n",
        "\n",
        "# 共起回数が2以上の単語のペアだけをノードとエッジとして追加\n",
        "for pair, freq in co_dict.items():\n",
        "    if freq >= 2:\n",
        "        G.add_node(pair[0])\n",
        "        G.add_node(pair[1])\n",
        "        G.add_edge(pair[0], pair[1], weight=freq)\n",
        "\n",
        "# 共起ネットワークを描画\n",
        "plt.figure(figsize=(10,10))\n",
        "pos = nx.spring_layout(G, k=0.3) # ノードの位置をばねモデルで決定\n",
        "nx.draw_networkx(G, pos, font_family=\"Meiryo\", font_size=16,\n",
        "                 node_color=\"skyblue\", edge_color=\"gray\", width=2) # ネットワークを描画\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KdWF-LilOokh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}